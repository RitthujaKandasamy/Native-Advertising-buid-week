{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>content</th>\n",
       "      <th>sponsored</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>It was a pleasure to welcome Jonnie Irwin and ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>My staff seem to be on Facebook, Twitter and o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>See our online help Get Support Here   2012 Al...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>obots\" content=\"noindex, follow\" /&gt; script typ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>Nearly a year ago to the day,Brewmeister brewe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>Gadajace Glowy (Talking Heads) is a serious do...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>A new study has come out that shows the Greek...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>BRASLIABrazils next finance minister, to be ap...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>Follow and Subscribe</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0  \\\n",
       "0             1           1   \n",
       "1             2           2   \n",
       "2             4           4   \n",
       "3             5           5   \n",
       "4             7           7   \n",
       "5             8           8   \n",
       "6             9           9   \n",
       "7            10          10   \n",
       "8            11          11   \n",
       "9            12          12   \n",
       "\n",
       "                                             content  sponsored  \n",
       "0  It was a pleasure to welcome Jonnie Irwin and ...          0  \n",
       "1  My staff seem to be on Facebook, Twitter and o...          0  \n",
       "2                                                             0  \n",
       "3  See our online help Get Support Here   2012 Al...          0  \n",
       "4  obots\" content=\"noindex, follow\" /> script typ...          0  \n",
       "5  Nearly a year ago to the day,Brewmeister brewe...          0  \n",
       "6  Gadajace Glowy (Talking Heads) is a serious do...          0  \n",
       "7   A new study has come out that shows the Greek...          0  \n",
       "8  BRASLIABrazils next finance minister, to be ap...          0  \n",
       "9                             Follow and Subscribe            0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# because the dataset is int tsv format we have to use delimeter.\n",
    "df = pd.read_csv(r'C:\\Users\\ritth\\code\\Data\\df_extracted.csv')\n",
    "\n",
    "# creating a copy so we don't messed up our original dataset.\n",
    "data=df.copy()\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>content</th>\n",
       "      <th>sponsored</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>It was a pleasure to welcome Jonnie Irwin and ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>My staff seem to be on Facebook, Twitter and o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>See our online help Get Support Here   2012 Al...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>obots\" content=\"noindex, follow\" /&gt; script typ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0  \\\n",
       "0             1           1   \n",
       "1             2           2   \n",
       "2             4           4   \n",
       "3             5           5   \n",
       "4             7           7   \n",
       "\n",
       "                                             content  sponsored  \n",
       "0  It was a pleasure to welcome Jonnie Irwin and ...          0  \n",
       "1  My staff seem to be on Facebook, Twitter and o...          0  \n",
       "2                                                             0  \n",
       "3  See our online help Get Support Here   2012 Al...          0  \n",
       "4  obots\" content=\"noindex, follow\" /> script typ...          0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data.drop(['Unnamed:0.1'],axis=1,inplace=True)\n",
    "sentences=data.content.values\n",
    "labels = data.sponsored.values\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  It was a pleasure to welcome Jonnie Irwin and his crew down to the brewery this month to film a piece for BBC2 s Escape to the Country . In each episode presenter Jonnie helps a couple who are keen to relocate to somewhere more rural and lines up several possible houses to suit their... The building of the Otter Brewery was, and still is, a labour of love. We believe that s why our beer tastes so great and why more and more people are drinking Otter Beer. From old time drinkers and die-hard landlords to new age drinkers, we have tried to create a range of beers that provides something for everyone. We hope this website gives a small taste of what we are about. A pint of our beer would do just as well - if not better! We hope youenjoy both. \n",
      "Tokenized:  ['it', 'was', 'a', 'pleasure', 'to', 'welcome', 'jon', '##nie', 'irwin', 'and', 'his', 'crew', 'down', 'to', 'the', 'brewery', 'this', 'month', 'to', 'film', 'a', 'piece', 'for', 'bbc', '##2', 's', 'escape', 'to', 'the', 'country', '.', 'in', 'each', 'episode', 'presenter', 'jon', '##nie', 'helps', 'a', 'couple', 'who', 'are', 'keen', 'to', 'relocate', 'to', 'somewhere', 'more', 'rural', 'and', 'lines', 'up', 'several', 'possible', 'houses', 'to', 'suit', 'their', '.', '.', '.', 'the', 'building', 'of', 'the', 'otter', 'brewery', 'was', ',', 'and', 'still', 'is', ',', 'a', 'labour', 'of', 'love', '.', 'we', 'believe', 'that', 's', 'why', 'our', 'beer', 'tastes', 'so', 'great', 'and', 'why', 'more', 'and', 'more', 'people', 'are', 'drinking', 'otter', 'beer', '.', 'from', 'old', 'time', 'drink', '##ers', 'and', 'die', '-', 'hard', 'landlord', '##s', 'to', 'new', 'age', 'drink', '##ers', ',', 'we', 'have', 'tried', 'to', 'create', 'a', 'range', 'of', 'beers', 'that', 'provides', 'something', 'for', 'everyone', '.', 'we', 'hope', 'this', 'website', 'gives', 'a', 'small', 'taste', 'of', 'what', 'we', 'are', 'about', '.', 'a', 'pin', '##t', 'of', 'our', 'beer', 'would', 'do', 'just', 'as', 'well', '-', 'if', 'not', 'better', '!', 'we', 'hope', 'you', '##en', '##joy', 'both', '.']\n",
      "Token IDs:  [2009, 2001, 1037, 5165, 2000, 6160, 6285, 8034, 17514, 1998, 2010, 3626, 2091, 2000, 1996, 12161, 2023, 3204, 2000, 2143, 1037, 3538, 2005, 4035, 2475, 1055, 4019, 2000, 1996, 2406, 1012, 1999, 2169, 2792, 10044, 6285, 8034, 7126, 1037, 3232, 2040, 2024, 10326, 2000, 20102, 2000, 4873, 2062, 3541, 1998, 3210, 2039, 2195, 2825, 3506, 2000, 4848, 2037, 1012, 1012, 1012, 1996, 2311, 1997, 1996, 22279, 12161, 2001, 1010, 1998, 2145, 2003, 1010, 1037, 4428, 1997, 2293, 1012, 2057, 2903, 2008, 1055, 2339, 2256, 5404, 16958, 2061, 2307, 1998, 2339, 2062, 1998, 2062, 2111, 2024, 5948, 22279, 5404, 1012, 2013, 2214, 2051, 4392, 2545, 1998, 3280, 1011, 2524, 18196, 2015, 2000, 2047, 2287, 4392, 2545, 1010, 2057, 2031, 2699, 2000, 3443, 1037, 2846, 1997, 18007, 2008, 3640, 2242, 2005, 3071, 1012, 2057, 3246, 2023, 4037, 3957, 1037, 2235, 5510, 1997, 2054, 2057, 2024, 2055, 1012, 1037, 9231, 2102, 1997, 2256, 5404, 2052, 2079, 2074, 2004, 2092, 1011, 2065, 2025, 2488, 999, 2057, 3246, 2017, 2368, 24793, 2119, 1012]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "# using the low level BERT for our task.\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# Printing the original sentence.\n",
    "print(' Original: ', sentences[0])\n",
    "\n",
    "# Printing the tokenized sentence in form of list.\n",
    "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
    "\n",
    "# Print the sentence mapped to token ids.\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5871 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "input_ids = []\n",
    "for sent in sentences:\n",
    "    # so basically encode tokenizing , mapping sentences to thier token ids after adding special tokens.\n",
    "    encoded_sent = tokenizer.encode(\n",
    "                        sent,                      # Sentence which are encoding.\n",
    "                        add_special_tokens = True, # Adding special tokens '[CLS]' and '[SEP]'\n",
    "\n",
    "                         )\n",
    "    \n",
    " \n",
    "    input_ids.append(encoded_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ritth\\code\\Strive\\Native-Advertising-buid-week\\rit_files\\model.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Native-Advertising-buid-week/rit_files/model.ipynb#ch0000004?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msequence\u001b[39;00m \u001b[39mimport\u001b[39;00m pad_sequences\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Native-Advertising-buid-week/rit_files/model.ipynb#ch0000004?line=2'>3</a>\u001b[0m MAX_LEN \u001b[39m=\u001b[39m \u001b[39m128\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Native-Advertising-buid-week/rit_files/model.ipynb#ch0000004?line=4'>5</a>\u001b[0m input_ids \u001b[39m=\u001b[39m pad_sequences(input_ids, maxlen\u001b[39m=\u001b[39mMAX_LEN , truncating\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ritth\\software\\anaconda\\envs\\deep\\lib\\site-packages\\keras\\__init__.py:21\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39m\"\"\"Implementation of the Keras API, the high-level API of TensorFlow.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[39mDetailed documentation and user guides are available at\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39m[keras.io](https://keras.io).\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39m# pylint: disable=unused-import\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m tf2\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m distribute\n\u001b[0;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m models\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_LEN = 128\n",
    "\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN , truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_masks = []\n",
    "\n",
    "for sent in input_ids:\n",
    "    \n",
    "    # Generating attention mask for sentences.\n",
    "    #   - when there is 0 present as token id we are going to set mask as 0.\n",
    "    #   - we are going to set mask 1 for all non-zero positive input id.\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]\n",
    "    \n",
    "   \n",
    "    attention_masks.append(att_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, test_size=0.2)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#changing the numpy arrays into tensors for working on GPU. \n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Deciding the batch size for training.\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "#DataLoader for our training set.\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# DataLoader for our validation(test) set.\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels = 2,   \n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, \n",
    "                  eps = 1e-8 \n",
    "                )\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "epochs = 1\n",
    "#import format_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_values = []\n",
    "test_losses = []\n",
    "test_acc = []\n",
    "\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    t0 = time.time()\n",
    "\n",
    "    total_loss = 0\n",
    "    running_test_loss = 0\n",
    "    running_test_acc = 0\n",
    "    # putting model in traing mode there are two model eval and train for model\n",
    "    model.train()\n",
    "    \n",
    "    \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        \n",
    "        #checking time taken after every 50 steps.\n",
    "        #if step % 50 == 0 and not step == 0:\n",
    "            #elapsed = format_time(time.time() - t0)\n",
    "        \n",
    "        #getting ids,mask,labes for every batch\n",
    "        b_input_ids = batch[0]\n",
    "        b_input_mask = batch[1]\n",
    "        b_labels = batch[2]\n",
    "        \n",
    "        \n",
    "        outputs = model(b_input_ids, \n",
    "                    token_type_ids=None, \n",
    "                    attention_mask=b_input_mask, \n",
    "                    labels=b_labels)\n",
    "\n",
    "        loss = outputs[0]\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # doing back propagation\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "    avg_train_loss = total_loss / len(train_dataloader) \n",
    "    loss_values.append(avg_train_loss)\n",
    "    \n",
    "    print(f'TrainLoss: {loss_values[-1] :.4f}')\n",
    "    \n",
    "   \n",
    " \n",
    "    model.eval()\n",
    "   \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(validation_dataloader):\n",
    "            b_input_ids = batch[0]\n",
    "            b_input_mask = batch[1]\n",
    "            b_labels = batch[2]\n",
    "            \n",
    "            \n",
    "            outputs = model(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask, \n",
    "                        labels=b_labels)\n",
    "\n",
    "            loss = outputs[0]\n",
    "\n",
    "            running_test_loss += loss.item()\n",
    "\n",
    "            # accuracy\n",
    "            targets = targets.int().numpy()\n",
    "            probs = torch.sigmoid(outputs).detach()\n",
    "            preds = (probs >= 0.5).int().numpy()\n",
    "\n",
    "            running_test_acc += accuracy_score(targets, preds)\n",
    "\n",
    "        test_acc.append(running_test_acc / len(validation_dataloader))\n",
    "        test_losses.append(running_test_loss / len(validation_dataloader))\n",
    "\n",
    "        print(f'TestLoss: {test_losses[-1] :.4f}')\n",
    "        print(f'TestAccu: {test_acc[-1] *100 :.2f}%')\n",
    "        print(f'Time: {time.time() - t0 :.4f}')\n",
    "\n",
    "\n",
    "        # save best model\n",
    "        if test_acc[-1] > benchmark_acc:\n",
    "            # save model to cpu\n",
    "            torch.save(model.state_dict(), './model.pth')\n",
    "            \n",
    "\n",
    "            # update benckmark\n",
    "            benchmark_acc = test_acc[-1]\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(loss_values, label= \"Train Loss\")\n",
    "plt.plot(test_losses, label= \"Test Loss\")\n",
    "plt.xlabel(\" Iteration \")\n",
    "plt.ylabel(\"Loss value\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('deep')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a6a404c1b23560d548308d831c1aa8041fb180aef1b35cf4a28ead3655e6085d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
